{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Github COPY INTO Dynamic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNc1JQ0hpvO8p9N8ZzGXx5N"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMER2B5R-h5A",
        "colab_type": "text"
      },
      "source": [
        "## Install Libraries using pip\n",
        "This must be done when using Colab Notebooks, since the instance of Python is not a permanent one.  Once all of the pip installs are done, execute the os._exit cell to restart your runtime.  This does not need to be done between pip installs as is prompted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKuXOXDLeyto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade snowflake-connector-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CC6KlPqfVxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFzjTPtufgd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade snowflake-sqlalchemy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Ig_-4tuIUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os._exit(00) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bddz7BgK_E52"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uL--XWAeS9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "from snowflake.sqlalchemy import URL\n",
        "import snowflake.connector\n",
        "import sqlalchemy as sa\n",
        "from sys import exit\n",
        "from sqlalchemy import text\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuE014LA77VK",
        "colab_type": "text"
      },
      "source": [
        "## Enter Connection Information\n",
        "Please enter the necessary connection information, as well as the parameters necessary to execute the file load.  Please note that the password MUST be Base64 Encoded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn7n8sVhu7Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Snowflake Credentials\n",
        "sourceUser = ''\n",
        "sourceAccount = ''\n",
        "sourcePassword = ''\n",
        "sourceDatabase = ''\n",
        "sourceSchema = ''\n",
        "sourceWarehouse = ''\n",
        "\n",
        "# Parameters\n",
        "db_name = ''\n",
        "schema_name = ''\n",
        "stage_name = ''\n",
        "format_name = ''\n",
        "warehouse_pattern = ''\n",
        "\n",
        "# Execution Mode (options: 'FAST_MODE', 'ECONOMY_MODE')\n",
        "# Economy mode requires 50% more files than a warehouse size can handle before\n",
        "# moving up to the next size.  This can actually be pretty efficient in\n",
        "# situations where files are varying sizes and some threads finish before\n",
        "# others.  It also tries to avoid under-utilization on larger warehouse sizes.\n",
        "exec_mode = 'FAST_MODE'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAmvAMtB9ekb",
        "colab_type": "text"
      },
      "source": [
        "## Functions and Connections\n",
        "Execute the following cell, which will compile the necessary functions of this Notebook, as well as create the necessary connections to Snowflake.  The output of the cell will be the current user, role, database, schema, warehouse, and version of Snowflake that you are connecting with.  Please validate these are correct before moving on to the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBEWCn13fGUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getEngine(snowUser=None, snowPassword=None, snowAccount=None, snowDatabase=None, snowSchema=None, snowWarehouse=None):\n",
        "    try:\n",
        "        base64_bytes = snowPassword.encode('ascii')\n",
        "        message_bytes = base64.b64decode(base64_bytes)\n",
        "        password = message_bytes.decode('ascii')\n",
        "\n",
        "        SnowEngine = sa.create_engine(URL(\n",
        "            user=snowUser,\n",
        "            password=password,\n",
        "            account=snowAccount,\n",
        "            database=snowDatabase,\n",
        "            schema=snowSchema,\n",
        "            warehouse=snowWarehouse\n",
        "        ))\n",
        "\n",
        "        return SnowEngine\n",
        "\n",
        "    except Exception as e:\n",
        "        errorStr = 'ERROR: ' + str(e)\n",
        "        return errorStr\n",
        "\n",
        "def execSQLDataFrame(SnowEngine, sqlQuery):\n",
        "    try:\n",
        "        if SnowEngine is None:\n",
        "            print('SnowEngine argument is required')\n",
        "            exit(InvalidArgsCode)\n",
        "\n",
        "        if sqlQuery is None:\n",
        "            print('sqlQuery argument is required')\n",
        "            exit(InvalidArgsCode)\n",
        "\n",
        "        resultSet = pd.read_sql(text(sqlQuery), SnowEngine)\n",
        " \n",
        "        return resultSet\n",
        "\n",
        "    except Exception as e:\n",
        "        errorStr = 'ERROR (execSQLDataFrame)' + str(e)\n",
        "        print(errorStr)\n",
        "        raise\n",
        "\n",
        "def execDBQuery(sqlQuery, con):\n",
        "    try:\n",
        "\n",
        "        if sqlQuery is None:\n",
        "            print('sqlQuery argument is required')\n",
        "            exit(InvalidArgsCode)\n",
        "\n",
        "        if con is None:\n",
        "            print('con argument is required')\n",
        "            exit(InvalidArgsCode)\n",
        "\n",
        "        cur = con.cursor()\n",
        "        qid = cur.execute(sqlQuery).sfqid\n",
        "        cur.close()\n",
        "\n",
        "        return qid\n",
        "\n",
        "    except Exception as e:\n",
        "        errorStr = 'ERROR (execDBQuery)' + str(e)\n",
        "        print(errorStr)\n",
        "        raise\n",
        "\n",
        "def execDBQueryAsync(sqlQuery, con, snowWarehouse):\n",
        "    try:\n",
        "\n",
        "        if sqlQuery is None:\n",
        "            print('sqlQuery argument is required')\n",
        "            exit(InvalidArgsCode)\n",
        "\n",
        "        if snowWarehouse is None:\n",
        "            print('snowWarehouse argument is required')\n",
        "            exit(InvalidArgsCode)\n",
        "\n",
        "        if con is None:\n",
        "            print('con argument is required')\n",
        "            exit(InvalidArgsCode)\n",
        "\n",
        "        whQuery = \"\"\"USE WAREHOUSE \"\"\" + snowWarehouse + \"\"\";\"\"\"\n",
        "\n",
        "        cura = con.cursor()\n",
        "        cura.execute(whQuery)\n",
        "        cura.execute(sqlQuery, _no_results=True)\n",
        "\n",
        "        time.sleep(5)\n",
        "        whQuery = \"\"\"ALTER WAREHOUSE \"\"\" + snowWarehouse + \"\"\" SUSPEND;\"\"\"\n",
        "        cura.execute(whQuery)\n",
        "\n",
        "    except Exception as e:\n",
        "        errorStr = 'ERROR (execDBQueryAsync)' + str(e)\n",
        "        print(errorStr)\n",
        "        raise\n",
        "\n",
        "\n",
        "base64_bytes = sourcePassword.encode('ascii')\n",
        "message_bytes = base64.b64decode(base64_bytes)\n",
        "password = message_bytes.decode('ascii')\n",
        "\n",
        "con = snowflake.connector.connect(\n",
        "    user=sourceUser,\n",
        "    password=password,\n",
        "    account=sourceAccount,\n",
        "    warehouse=sourceWarehouse,\n",
        "    database=sourceDatabase,\n",
        "    snowSchema=sourceSchema\n",
        ")\n",
        "\n",
        "cona = snowflake.connector.connect(\n",
        "    user=sourceUser,\n",
        "    password=password,\n",
        "    account=sourceAccount,\n",
        "    warehouse=sourceWarehouse,\n",
        "    database=sourceDatabase,\n",
        "    snowSchema=sourceSchema\n",
        ")\n",
        "\n",
        "SnowEngine = getEngine(snowUser=sourceUser, snowPassword=sourcePassword, snowAccount=sourceAccount, snowDatabase=sourceDatabase, snowSchema=sourceSchema, snowWarehouse=sourceWarehouse)\n",
        "\n",
        "sqlQuery = \"\"\"SELECT CURRENT_USER() as user, CURRENT_ROLE() as role, CURRENT_DATABASE() as db, CURRENT_SCHEMA() as schema, CURRENT_WAREHOUSE() as wh, CURRENT_VERSION() as ver;\"\"\"\n",
        "print(execSQLDataFrame(SnowEngine, sqlQuery))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rECzwXnK_OJH",
        "colab_type": "text"
      },
      "source": [
        "## Tables to Load\n",
        "Modify the SQL statement below to specify the tables that you wish to load during the execution of this script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75TSauhOUzEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqlQuery = \"\"\"\n",
        "    SELECT * \n",
        "    FROM \"\"\" + db_name + \"\"\".information_schema.tables\n",
        "    WHERE table_schema = '\"\"\" + schema_name.upper() + \"\"\"'\n",
        "      AND table_name LIKE 'DATA_LOAD_TEST%'\n",
        "    \"\"\"\n",
        "\n",
        "dfTables = execSQLDataFrame(SnowEngine, sqlQuery)\n",
        "\n",
        "dfTables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRkpIYOEAPpm",
        "colab_type": "text"
      },
      "source": [
        "## Main Execution\n",
        "The following cell is the main execution and will being executing COPY INTO statements across all available warehouses (mostly) in parallel.  The more warehouses available, the faster the tables will be executed.\n",
        "\n",
        "### Before Executing!!!\n",
        "There are several places within the following script that should be reviewed, as they are likely custom per customer, schema, data file, etc.  Look for the comments in the section below for more details.  Do Not modify any section that isn't between comment blocks unless you know what you're doing!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH8oLN-Y_DWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x,y in dfTables.iterrows():\n",
        "\n",
        "    dfWH = pd.DataFrame()\n",
        "\n",
        "    while (dfWH.shape[0] == 0):\n",
        "        print(str(time.strftime(\"%H:%M:%S\", time.localtime())) + ': Looking for available warehouses ... ')\n",
        "        \n",
        "# ----------------- WAREHOUSE SEARCH CUSTOMIZATION BEGIN ---------------------\n",
        "\n",
        "        WHQuery = \"\"\"\n",
        "            SHOW WAREHOUSES LIKE '\"\"\" + warehouse_pattern + \"\"\"%';\"\"\"\n",
        "\n",
        "# ----------------- WAREHOUSE SEARCH CUSTOMIZATION END -----------------------\n",
        "        \n",
        "        qid = execDBQuery(sqlQuery=WHQuery, con=con)\n",
        "        \n",
        "        WHQuery = \"\"\"\n",
        "            SELECT \"name\" as wh_name, \"state\" as state\n",
        "            FROM table(result_scan('\"\"\" + qid + \"\"\"'))\n",
        "            WHERE state = 'SUSPENDED';\n",
        "            \"\"\"\n",
        "        dfWH = execSQLDataFrame(SnowEngine, WHQuery)\n",
        "\n",
        "        if dfWH.shape[0] == 0:\n",
        "            time.sleep(10)\n",
        "\n",
        "\n",
        "# ----------------- FIND FILES CUSTOMIZATION BEGIN ---------------------------\n",
        "# Base Code assumes that the files are located in the root of internal table\n",
        "# stages.  This code does not use the stage_name parameter that is set earlier,\n",
        "# but it could.  Modify this code to find the files that are to be loaded.\n",
        "\n",
        "    sqlQuery = \"\"\"LIST @\"\"\" + sourceSchema + \"\"\".%\"\"\" + dfTables.table_name[x] + \"\"\";\"\"\"\n",
        "\n",
        "# ----------------- FIND FILES CUSTOMIZATION END -----------------------------\n",
        "\n",
        "    qid = execDBQuery(sqlQuery=sqlQuery, con=con)\n",
        "\n",
        "# ----------------- FILE FILTER CUSTOMIZATION BEGIN --------------------------\n",
        "# Base Code assumes that every file in the stage will be loaded. If this needs\n",
        "# to be customized, leverage the SQL block below.\n",
        "\n",
        "    cntQuery = \"\"\"\n",
        "        SELECT *\n",
        "        FROM table(result_scan('\"\"\" + qid + \"\"\"'));\n",
        "        \"\"\"\n",
        "# ----------------- FILE FILTER CUSTOMIZATION END ----------------------------\n",
        "    \n",
        "    dfFiles = execSQLDataFrame(SnowEngine, cntQuery)\n",
        "\n",
        "    fileCnt = dfFiles.shape[0]\n",
        "\n",
        "    if exec_mode == 'FAST_MODE':\n",
        "        if fileCnt <= 8:\n",
        "            whSize = 'XSMALL'\n",
        "        elif fileCnt > 8 and fileCnt <= 16:\n",
        "            whSize = 'SMALL'\n",
        "        elif fileCnt > 16 and fileCnt <= 32:\n",
        "            whSize = 'MEDIUM'\n",
        "        elif fileCnt > 32 and fileCnt <= 64:\n",
        "            whSize = 'LARGE'\n",
        "        elif fileCnt > 64 and fileCnt <= 128:\n",
        "            whSize = 'XLARGE'        \n",
        "        elif fileCnt > 128 and fileCnt <= 256:\n",
        "            whSize = 'XXLARGE'\n",
        "        elif fileCnt > 256 and fileCnt <= 512:\n",
        "            whSize = 'XXXLARGE'\n",
        "        elif fileCnt > 512:\n",
        "            whSize = 'X4LARGE'\n",
        "    else:\n",
        "        if fileCnt <= 12:\n",
        "            whSize = 'XSMALL'\n",
        "        elif fileCnt > 12 and fileCnt <= 24:\n",
        "            whSize = 'SMALL'\n",
        "        elif fileCnt > 24 and fileCnt <= 48:\n",
        "            whSize = 'MEDIUM'\n",
        "        elif fileCnt > 48 and fileCnt <= 96:\n",
        "            whSize = 'LARGE'\n",
        "        elif fileCnt > 96 and fileCnt <= 192:\n",
        "            whSize = 'XLARGE'        \n",
        "        elif fileCnt > 192 and fileCnt <= 384:\n",
        "            whSize = 'XXLARGE'\n",
        "        elif fileCnt > 384 and fileCnt <= 728:\n",
        "            whSize = 'XXXLARGE'\n",
        "        elif fileCnt > 728:\n",
        "            whSize = 'X4LARGE'\n",
        "\n",
        "    sqlQuery = \"\"\"\n",
        "    ALTER WAREHOUSE \"\"\" + dfWH.wh_name[0] + \"\"\" \n",
        "    SET WAREHOUSE_SIZE = \"\"\" + whSize + \"\"\";\"\"\"\n",
        " \n",
        "    qid = execDBQuery(sqlQuery=sqlQuery, con=con)\n",
        "    print(str(time.strftime(\"%H:%M:%S\", time.localtime())) + ': Warehouse set to: ' + whSize)\n",
        "\n",
        "    sqlQuery = \"\"\"\n",
        "    ALTER WAREHOUSE \"\"\" + dfWH.wh_name[0] + \"\"\" \n",
        "    RESUME;\"\"\"\n",
        " \n",
        "    qid = execDBQuery(sqlQuery=sqlQuery, con=con)\n",
        "    print(str(time.strftime(\"%H:%M:%S\", time.localtime())) + ': Warehouse ' + dfWH.wh_name[0] + ' Resumed')\n",
        "\n",
        "    time.sleep(5)\n",
        "\n",
        "# ----------------- COPY INTO CUSTOMIZATION BEGIN ----------------------------\n",
        "# Base Code assumes that every file in the stage will be loaded. If additional\n",
        "# parameters need to be added to the COPY statement, or the file selection\n",
        "# filter needs to be customized, leverage the SQL block below.\n",
        "\n",
        "    sqlQuery = \"\"\"\n",
        "        COPY INTO \"\"\" + dfTables.table_name[x] + \"\"\" \n",
        "        FROM @\"\"\" + sourceSchema.lower() + \"\"\".%\"\"\" + dfTables.table_name[x].lower() + \"\"\"/\n",
        "        PATTERN = '.*[.]csv[.]gz'\n",
        "        FILE_FORMAT = (format_name = \"\"\" + format_name + \"\"\");\n",
        "        \"\"\"\n",
        "# ----------------- COPY INTO CUSTOMIZATION END ------------------------------\n",
        "    \n",
        "    execDBQueryAsync(sqlQuery=sqlQuery, con=cona, snowWarehouse=dfWH.wh_name[0])\n",
        "    print(str(time.strftime(\"%H:%M:%S\", time.localtime())) + ': Copy Into for table: ' + dfTables.table_name[x])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDfrKGbi9_f3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}